{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg/dvj6Mzm6U+K20EyQzIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedjialeuJordan/Machine-Learning-Projects/blob/main/Rating_prep_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hotel prediction model"
      ],
      "metadata": {
        "id": "42YvDGcC32LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "train_df = pd.read_csv('/content/train_hotel_reviews.csv')\n",
        "val_df = pd.read_csv('/content/valid_hotel_reviews.csv')\n",
        "\n",
        "#converti le texte en minuscules et exrait les mots via une expression regulière\n",
        "def tokenize(text):\n",
        "  text=text.lower()\n",
        "  tokens= re.findall(r'\\b\\w+\\b',text)\n",
        "  return tokens\n",
        "#creation d'un vocabulaire basé sur les plus mots les plus frequent du corpus d'entrainement\n",
        "counter = Counter()\n",
        "for review in train_df['Review']:\n",
        "  counter.update(tokenize(review))\n",
        "\n",
        "#Indexation du vocabulaire\n",
        "vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common(10000))}\n",
        "vocab['<pad>'] = 0 #padding\n",
        "vocab['<unk>'] = 1 #mots inconnu\n",
        "\n",
        "#encodage du texte en indice numerique selon le vocabulaire\n",
        "def encode(text):\n",
        "  tokens = tokenize(text)\n",
        "  encoded = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "  return encoded\n",
        "\n",
        "#dataset pour les critiques d'hotels\n",
        "class HotelDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.reviews=[torch.tensor(encode(text)) for text in df['Review']] #encodage critique\n",
        "    self.ratings=torch.tensor(df['Rating'].values) - 1 #encodage des notes\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews) #nombre total d'echantillons\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    review = self.reviews[idx]\n",
        "    rating = self.ratings[idx]\n",
        "    return review, rating\n",
        "\n",
        "  #fonction pour creer des batch avec padding\n",
        "  def collate_fn(batch):\n",
        "    texts, labels= zip(*batch)\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=0)#permet d'avoir les sequences pour avoir la meme longueur dans chaque batch\n",
        "    return texts, torch.tensor(labels)\n",
        "\n",
        "#Creation des dataloaders pour l'entrainement et la validation\n",
        "train_dataset = HotelDataset(train_df)\n",
        "val_dataset = HotelDataset(val_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=HotelDataset.collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=HotelDataset.collate_fn)\n",
        "\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, num_classes=5):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)#Transforme les indices en vecteurs\n",
        "    self.bilstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "    self.fc = nn.Linear(hidden_dim*2, num_classes)#transforme la sortie de LSTM en scores de classes\n",
        "\n",
        "  def forward(self,x):\n",
        "    embedded=self.embedding(x)\n",
        "    _, (hidden, _) = self.bilstm(embedded) #on recupère juste le hidden de la diRection avant et arriere\n",
        "    #Concatenation des deux derniers états cachés(foward -2 et backward -1)\n",
        "    hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "    output = self.fc(hidden)\n",
        "    return output\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = len(vocab)\n",
        "model = BiLSTMModel(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#optimiseur Adam\n",
        "\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  for texts, labels in train_loader:\n",
        "    texts, labels = texts.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(texts)\n",
        "    loss = criterion(outputs, labels)#calcule de la loss\n",
        "    loss.backward()#backpropagation: calcul du gradient\n",
        "    optimizer.step()#mise a jour des poids\n",
        "  print(f'Epoch {epoch+1} complete, Loss: {loss.item():.4f}')\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return total_loss / len(val_loader), accuracy\n",
        "\n",
        "val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
        "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "-HTM3_mR312w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d7f14a-36fa-4a65-88db-b948a263403f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete, Loss: 1.1124\n",
            "Epoch 2 complete, Loss: 0.6410\n",
            "Epoch 3 complete, Loss: 0.6628\n",
            "Epoch 4 complete, Loss: 0.7755\n",
            "Epoch 5 complete, Loss: 0.7075\n",
            "Epoch 6 complete, Loss: 0.4046\n",
            "Epoch 7 complete, Loss: 0.8428\n",
            "Epoch 8 complete, Loss: 0.3166\n",
            "Epoch 9 complete, Loss: 0.1970\n",
            "Epoch 10 complete, Loss: 0.1952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du fichier test\n",
        "test_df = pd.read_csv('/content/test_hotel_reviews.csv')\n",
        "\n",
        "# Création d'un dataset de test\n",
        "class HotelTestDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.reviews = [torch.tensor(encode(text)) for text in df['Review']]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.reviews[idx]\n",
        "\n",
        "  def collate_fn(batch):\n",
        "    return pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "\n",
        "# Préparation du DataLoader\n",
        "test_dataset = HotelTestDataset(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=HotelTestDataset.collate_fn)\n",
        "\n",
        "# Prédiction\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "  for texts in test_loader:\n",
        "    texts = texts.to(device)\n",
        "    outputs = model(texts)\n",
        "    preds = outputs.argmax(dim=1)\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "def evaluate_test(model, df, criterion):\n",
        "    dataset = HotelDataset(df)  # On réutilise la classe d'entraînement\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=HotelDataset.collate_fn)\n",
        "    return evaluate(model, loader, criterion, device)\n",
        "\n",
        "# Ajout des prédictions à la DataFrame\n",
        "test_df['Predicted Rating'] = [p + 1 for p in all_preds]  # On remet de 1 à 5\n",
        "test_df.to_csv(\"test_predictions.csv\", index=False)\n",
        "test_loss, test_accuracy = evaluate_test(model, test_df, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f} | Accuracy: {test_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "akfQAFSm38FJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63511bd-a951-42b9-8ca2-6a42bf89b528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.4661 | Accuracy: 61.40%\n"
          ]
        }
      ]
    }
  ]
}